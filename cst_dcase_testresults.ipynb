{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb358fe-3242-47d0-a24a-1a6aadc47142",
   "metadata": {},
   "source": [
    "# Cycle Self Training Implementation on DCASE TAU 2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e86ed20-4362-4f67-80ef-33cf50eddd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import argparse\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import librosa\n",
    "from hear21passt.base import get_basic_model\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8ff3da-dd18-4d39-8bdf-5aff96709d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import common.audio.datasets as datasets\n",
    "from common.audio.transforms import TransformFixMatch\n",
    "from common.audio.transforms import WeakAugment\n",
    "from common.audio.transforms import StrongAugment\n",
    "from common.utils.data import ForeverDataIterator\n",
    "from common.utils.metric import accuracy, ConfusionMatrix\n",
    "from common.utils.meter import AverageMeter, ProgressMeter\n",
    "from common.utils.logger import CompleteLogger\n",
    "from common.utils.analysis import collect_feature, tsne, a_distance\n",
    "from common.tools.randaugment import rand_augment_transform, GaussianBlur\n",
    "from common.tools.sam import SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902e6538-8731-4ca0-a428-1d9532f64388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4c41f7-5bb7-4a4e-9fc5-2dfbb9b277a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments set for CST experiment (DCASE TAU 2020):\n",
      "root = ./data/dcase\n",
      "data = DCASE\n",
      "source = source\n",
      "target = target\n",
      "temperature = 2.0\n",
      "alpha = 1.9\n",
      "trade_off = 0.08\n",
      "trade_off1 = 0.5\n",
      "trade_off3 = 0.5\n",
      "threshold = 0.97\n",
      "rho = 0.5\n",
      "batch_size = 3\n",
      "lr = 0.0005\n",
      "lr_gamma = 0.001\n",
      "lr_decay = 0.75\n",
      "momentum = 0.9\n",
      "weight_decay = 0.001\n",
      "workers = 2\n",
      "epochs = 50\n",
      "early = 45\n",
      "iters_per_epoch = 1000\n",
      "val_interval = 10\n",
      "print_freq = 100\n",
      "seed = None\n",
      "per_class_eval = False\n",
      "log = logs/dcase\n",
      "phase = test\n",
      "sample_rate = 32000\n",
      "clip_length = 10\n",
      "num_cls = 10\n",
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    root = './data/dcase',\n",
    "    data = 'DCASE',          \n",
    "    source = 'source',      \n",
    "    target = 'target',\n",
    "    temperature = 2.0,\n",
    "    alpha = 1.9,\n",
    "    trade_off = 0.08,\n",
    "    trade_off1 = 0.5,\n",
    "    trade_off3 = 0.5,\n",
    "    threshold = 0.97,\n",
    "    rho = 0.5,\n",
    "    batch_size = 3,\n",
    "    lr = 0.0005,\n",
    "    lr_gamma = 0.001,\n",
    "    lr_decay = 0.75,\n",
    "    momentum = 0.9,\n",
    "    weight_decay = 1e-3,\n",
    "    workers = 2,\n",
    "    epochs = 50,\n",
    "    early = 45,\n",
    "    iters_per_epoch = 1000,\n",
    "    val_interval = 10,\n",
    "    print_freq = 100,        \n",
    "    seed = None,             \n",
    "    per_class_eval = False,  \n",
    "    log = 'logs/dcase',             \n",
    "    phase = 'test', \n",
    "    sample_rate = 32000,\n",
    "    clip_length = 10,\n",
    "    num_cls = 10,\n",
    "    device = device,\n",
    "    \n",
    ")\n",
    "print(\"Arguments set for CST experiment (DCASE TAU 2020):\")\n",
    "for k, v in args.__dict__.items():\n",
    "    print(f\"{k} = {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ca24ac-0755-4100-85e6-72bfb953f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(predictions: torch.Tensor, reduction='none') -> torch.Tensor:\n",
    "    epsilon = 1e-5\n",
    "    H = -predictions * torch.log(predictions + epsilon)\n",
    "    H = H.sum(dim=1)\n",
    "    if reduction == 'mean':\n",
    "        return H.mean()\n",
    "    else:\n",
    "        return H\n",
    "\n",
    "class TsallisEntropy(nn.Module):\n",
    "    def __init__(self, temperature: float, alpha: float):\n",
    "        super(TsallisEntropy, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        N, C = logits.shape\n",
    "        pred = F.softmax(logits / self.temperature, dim=1) \n",
    "        entropy_weight = entropy(pred).detach()\n",
    "        entropy_weight = 1 + torch.exp(-entropy_weight)\n",
    "        entropy_weight = (N * entropy_weight / torch.sum(entropy_weight)).unsqueeze(dim=1)  \n",
    "        sum_dim = torch.sum(pred * entropy_weight, dim = 0).unsqueeze(dim=0)\n",
    "        return 1 / (self.alpha - 1) * torch.sum((1 / torch.mean(sum_dim) - torch.sum(pred ** self.alpha / sum_dim * entropy_weight, dim = -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc43990-595a-40de-a9b8-28c12a583d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier (nn.Module):\n",
    "    def __init__(self, backbone, num_classes, bottleneck_dim=256, finetune=True):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.backbone = backbone  \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(768, bottleneck_dim),  \n",
    "            nn.BatchNorm1d(bottleneck_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.head = nn.Linear(bottleneck_dim, num_classes)\n",
    "        self.finetune = finetune\n",
    "\n",
    "        if not finetune:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.bottleneck(features)\n",
    "        outputs = self.head(embeddings)\n",
    "        return outputs, embeddings\n",
    "\n",
    "    def get_parameters(self, base_lr=1.0) -> List[Dict]:\n",
    "        \"\"\"A parameter list which decides optimization hyper-parameters,\n",
    "            such as the relative learning rate of each layer\n",
    "        \"\"\"\n",
    "        params = [\n",
    "            {\"params\": self.backbone.parameters(), \"lr\": 0.1 * base_lr if self.finetune else 1.0 * base_lr},\n",
    "            {\"params\": self.bottleneck.parameters(), \"lr\": 1.0 * base_lr},\n",
    "            {\"params\": self.head.parameters(), \"lr\": 1.0 * base_lr},\n",
    "        ]\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03152f09-be36-4620-b490-9eb8690b7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train source samples: 9193\n",
      "Train target samples: 3372\n",
      "Val source samples: 1022\n",
      "Val target samples: 375\n",
      "Test source samples: 330\n",
      "Test target samples: 2638\n"
     ]
    }
   ],
   "source": [
    "# Data loading code\n",
    "train_transform = WeakAugment()\n",
    "unlabeled_transform = TransformFixMatch()\n",
    "val_transform = WeakAugment()  \n",
    "\n",
    "dataset = datasets.__dict__[args.data]\n",
    "\n",
    "train_source_dataset = dataset(root=args.root, task=args.source, split='train', transform=train_transform)\n",
    "train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last=True)\n",
    "\n",
    "train_target_dataset = dataset(root=args.root, task=args.target, split='train', transform=unlabeled_transform)\n",
    "train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last=True)\n",
    "    \n",
    "train_size = int(0.9 * len(train_source_dataset))\n",
    "val_size = len(train_source_dataset) - train_size\n",
    "train_source_dataset, val_source_dataset = torch.utils.data.random_split(train_source_dataset, [train_size, val_size])\n",
    "\n",
    "train_size = int(0.9 * len(train_target_dataset))\n",
    "val_size = len(train_target_dataset) - train_size\n",
    "train_target_dataset, val_target_dataset = torch.utils.data.random_split(train_target_dataset, [train_size, val_size])\n",
    "\n",
    "test_source_dataset = dataset(root=args.root, task=args.source, split='test', transform=val_transform)\n",
    "test_source_loader = DataLoader(test_source_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "test_target_dataset = dataset(root=args.root, task=args.target, split='test', transform=val_transform)\n",
    "test_target_loader = DataLoader(test_target_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "train_source_iter = ForeverDataIterator(train_source_loader)\n",
    "train_target_iter = ForeverDataIterator(train_target_loader)\n",
    "\n",
    "print(f\"Train source samples: {len(train_source_dataset)}\")\n",
    "print(f\"Train target samples: {len(train_target_dataset)}\")\n",
    "print(f\"Val source samples: {len(val_source_dataset)}\")\n",
    "print(f\"Val target samples: {len(val_target_dataset)}\")\n",
    "print(f\"Test source samples: {len(test_source_dataset)}\")\n",
    "print(f\"Test target samples: {len(test_target_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2517c4e8-8a77-4814-b01a-5052d8af937b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model PaSST\n",
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "print(\"=> using pre-trained model PaSST\")\n",
    "backbone = get_basic_model(mode=\"embed_only\")\n",
    "backbone.eval() \n",
    "classifier = AudioClassifier(backbone, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a493046-e2f5-4d9b-9fe9-561a630b2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args: argparse.Namespace):\n",
    "    logger = CompleteLogger(args.log, args.phase)\n",
    "    print(args)\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    # define optimizer and lr scheduler\n",
    "    base_optimizer = SGD\n",
    "    optimizer = SAM(classifier.get_parameters(), base_optimizer, lr = args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay, adaptive = True, rho = args.rho)\n",
    "    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))\n",
    "\n",
    "    # define loss function\n",
    "    ts_loss = TsallisEntropy(temperature=args.temperature, alpha = args.alpha)\n",
    "\n",
    "    # resume from the best checkpoint\n",
    "    if args.phase != 'train':\n",
    "        checkpoint = torch.load(logger.get_checkpoint_path('best'), map_location='cpu')\n",
    "        classifier.load_state_dict(checkpoint)\n",
    "\n",
    "    # analysis the model\n",
    "    if args.phase == 'analysis':\n",
    "        # extract features from both domains\n",
    "        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)\n",
    "        source_feature = collect_feature(train_source_loader, feature_extractor, device)\n",
    "        target_feature = collect_feature(train_target_loader, feature_extractor, device)\n",
    "        # plot t-SNE\n",
    "        tSNE_filename = osp.join(logger.visualize_directory, 'TSNE.png')\n",
    "        tsne.visualize(source_feature, target_feature, tSNE_filename)\n",
    "        print(\"Saving t-SNE to\", tSNE_filename)\n",
    "        # calculate A-distance, which is a measure for distribution discrepancy\n",
    "        A_distance = a_distance.calculate(source_feature, target_feature, device)\n",
    "        print(\"A-distance =\", A_distance)\n",
    "        return\n",
    "\n",
    "    if args.phase == 'test':\n",
    "        acc_s, _ = validate(test_source_loader, classifier, args)\n",
    "        print(acc_s)\n",
    "\n",
    "        acc_t, _ = validate(test_target_loader, classifier, args)\n",
    "        print(acc_t)\n",
    "        return\n",
    "\n",
    "    # start training\n",
    "    best_acc_s = 0.\n",
    "    best_acc_t = 0.\n",
    "    for epoch in range(min(args.epochs, args.early)):\n",
    "        print(\"lr:\", lr_scheduler.get_last_lr()[0])\n",
    "        # train for one epoch\n",
    "        train(train_source_iter, train_target_iter, classifier, ts_loss, optimizer,\n",
    "              lr_scheduler, epoch, args)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        if (epoch + 1) % args.val_interval == 0 or epoch == args.epochs - 1:\n",
    "            acc_s, device_results_s = validate(val_source_loader, classifier, args)\n",
    "            acc_t, device_results_t = validate(val_target_loader, classifier, args)\n",
    "            \n",
    "    \n",
    "            # remember best acc@1 and save checkpoint\n",
    "            torch.save(classifier.state_dict(), logger.get_checkpoint_path('latest'))\n",
    "            if acc_s > best_acc_s:\n",
    "                shutil.copy(logger.get_checkpoint_path('latest'), logger.get_checkpoint_path('best'))\n",
    "            best_acc_s = max(acc_s, best_acc_s)\n",
    "    \n",
    "            if acc_t > best_acc_t:\n",
    "                shutil.copy(logger.get_checkpoint_path('latest'), logger.get_checkpoint_path('best'))\n",
    "            best_acc_t = max(acc_t, best_acc_t)\n",
    "\n",
    "            print(\"Best Source Accuracy = {:3.1f}\".format(best_acc_s))\n",
    "            print(\"Best Target Accuracy = {:3.1f}\".format(best_acc_t))\n",
    "\n",
    "        else:\n",
    "            torch.save(classifier.state_dict(), logger.get_checkpoint_path('latest'))\n",
    "\n",
    "    \n",
    "\n",
    "    # evaluate on test set\n",
    "    classifier.load_state_dict(torch.load(logger.get_checkpoint_path('best')))\n",
    "    acc_s, _ = validate(test_source_loader, classifier, args)\n",
    "    print(\"Test Source Accuracy = {:3.1f}\".format(acc_s))\n",
    "\n",
    "    acc_t, _ = validate(test_target_loader, classifier, args)\n",
    "    print(\"Test Target Accuracy = {:3.1f}\".format(acc_t))\n",
    "\n",
    "    logger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee50557-88e9-47f2-89e9-2d3bb90d2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_source_iter: ForeverDataIterator, train_target_iter: ForeverDataIterator,\n",
    "          model: AudioClassifier, ts: TsallisEntropy, optimizer: SGD,\n",
    "          lr_scheduler: LambdaLR, epoch: int, args: argparse.Namespace):\n",
    "    batch_time = AverageMeter('Time', ':3.1f')\n",
    "    data_time = AverageMeter('Data', ':3.1f')\n",
    "    losses = AverageMeter('Loss', ':3.2f')\n",
    "    trans_losses = AverageMeter('Trans Loss', ':3.2f')\n",
    "    rev_losses = AverageMeter('CST Loss', ':3.2f')\n",
    "    fix_losses = AverageMeter('Fix Loss', ':3.2f')\n",
    "    cls_accs = AverageMeter('Cls Acc', ':3.1f')\n",
    "\n",
    "    progress = ProgressMeter(\n",
    "        args.iters_per_epoch,\n",
    "        [batch_time, data_time, losses, trans_losses, rev_losses, fix_losses, cls_accs],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(args.iters_per_epoch):\n",
    "        x_s, labels_s, _, _ = next(train_source_iter)\n",
    "        (x_t, x_t_u), _, _, _ = next(train_target_iter)\n",
    "\n",
    "        x_s = x_s.to(device)\n",
    "        x_t = x_t.to(device)\n",
    "        x_t_u = x_t_u.to(device)\n",
    "        labels_s = labels_s.to(device)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        x = torch.cat((x_s, x_t), dim=0)\n",
    "        y, f = model(x)\n",
    "        y_t_u, _ = model(x_t_u)\n",
    "\n",
    "        f_s, f_t = f.chunk(2, dim=0)\n",
    "        y_s, y_t = y.chunk(2, dim=0)\n",
    "\n",
    "        # generate target pseudo-labels\n",
    "        max_prob, pred_u = torch.max(F.softmax(y_t), dim=-1)\n",
    "        Lu = (F.cross_entropy(y_t_u, pred_u,\n",
    "                              reduction='none') * max_prob.ge(args.threshold).float().detach()).mean()\n",
    "\n",
    "        # compute cst\n",
    "        target_data_train_r = f_t\n",
    "        target_data_train_r = target_data_train_r / (torch.norm(target_data_train_r, dim = -1).reshape(target_data_train_r.shape[0], 1))\n",
    "        target_data_test_r = f_s\n",
    "        target_data_test_r = target_data_test_r / (torch.norm(target_data_test_r, dim = -1).reshape(target_data_test_r.shape[0], 1))\n",
    "        target_gram_r = torch.clamp(target_data_train_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "        target_kernel_r = target_gram_r\n",
    "        test_gram_r = torch.clamp(target_data_test_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "        test_kernel_r = test_gram_r\n",
    "        target_train_label_r = torch.nn.functional.one_hot(pred_u, args.num_cls) - 1 / float(args.num_cls) \n",
    "        target_test_pred_r = test_kernel_r.mm(torch.inverse(target_kernel_r + 0.001 * torch.eye(args.batch_size).cuda())).mm(target_train_label_r)\n",
    "        reverse_loss = nn.MSELoss()(target_test_pred_r, torch.nn.functional.one_hot(labels_s, args.num_cls) - 1 / float(args.num_cls)) \n",
    "\n",
    "        cls_loss = F.cross_entropy(y_s, labels_s)\n",
    "        transfer_loss = ts(y_t)\n",
    "\n",
    "        if Lu != 0:\n",
    "            loss = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1 + Lu * args.trade_off3\n",
    "        else: \n",
    "            loss = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1\n",
    "\n",
    "        cls_acc = accuracy(y_s, labels_s)[0]\n",
    "\n",
    "        losses.update(loss.item(), x_s.size(0))\n",
    "        cls_accs.update(cls_acc.item(), x_s.size(0))\n",
    "        trans_losses.update(transfer_loss.item(), x_s.size(0))\n",
    "        rev_losses.update(reverse_loss.item(), x_s.size(0))\n",
    "        fix_losses.update(Lu.item(), x_s.size(0))\n",
    "\n",
    "        # compute gradient and do the first SGD step\n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # compute gradient and do the second SGD step\n",
    "        y, f = model(x)\n",
    "        y_t_u, _ = model(x_t_u)\n",
    "\n",
    "        f_s, f_t = f.chunk(2, dim=0)\n",
    "        y_s, y_t = y.chunk(2, dim=0)\n",
    "\n",
    "        # generate target pseudo-labels\n",
    "        max_prob, pred_u = torch.max(F.softmax(y_t), dim=-1)\n",
    "        Lu = (F.cross_entropy(y_t_u, pred_u,\n",
    "                              reduction='none') * max_prob.ge(args.threshold).float().detach()).mean()\n",
    "\n",
    "        # compute cst\n",
    "        target_data_train_r = f_t\n",
    "        target_data_train_r = target_data_train_r / (torch.norm(target_data_train_r, dim = -1).reshape(target_data_train_r.shape[0], 1))\n",
    "        target_data_test_r = f_s\n",
    "        target_data_test_r = target_data_test_r / (torch.norm(target_data_test_r, dim = -1).reshape(target_data_test_r.shape[0], 1))\n",
    "        target_gram_r = torch.clamp(target_data_train_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "        target_kernel_r = target_gram_r\n",
    "        test_gram_r = torch.clamp(target_data_test_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "        test_kernel_r = test_gram_r\n",
    "        target_train_label_r = torch.nn.functional.one_hot(pred_u, args.num_cls) - 1 / float(args.num_cls) \n",
    "        target_test_pred_r = test_kernel_r.mm(torch.inverse(target_kernel_r + 0.001 * torch.eye(args.batch_size).cuda())).mm(target_train_label_r)\n",
    "        reverse_loss = nn.MSELoss()(target_test_pred_r, torch.nn.functional.one_hot(labels_s, args.num_cls) - 1 / float(args.num_cls)) \n",
    "\n",
    "        cls_loss = F.cross_entropy(y_s, labels_s)\n",
    "        transfer_loss = ts(y_t)\n",
    "\n",
    "        if Lu != 0:\n",
    "            loss1 = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1 + Lu * args.trade_off3\n",
    "        else: \n",
    "            loss1 = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1\n",
    "\n",
    "        loss1.backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            progress.display(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92762d86-1804-45cd-ba17-a2df7fd42bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_device(model, test_target_loader, device):\n",
    "    model.eval()\n",
    "    device_results = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for audio, labels, domains, devices in test_target_loader:\n",
    "            audio = audio.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(audio)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            for i, dev in enumerate(devices):\n",
    "                device_results[dev]['total'] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    device_results[dev]['correct'] += 1\n",
    "    \n",
    "    results = {}\n",
    "    for dev in device_results:\n",
    "        correct = device_results[dev]['correct']\n",
    "        total = device_results[dev]['total']\n",
    "        accuracy = 100.0 * correct / total\n",
    "        results[dev] = {'accuracy': accuracy, 'total': total}\n",
    "        print(f'Device {dev}: Accuracy = {accuracy:.2f}% ({correct}/{total})')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df9e46d-97d8-42d7-84a0-2d6b5f284477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader: DataLoader, model: AudioClassifier, args: argparse.Namespace) -> float:\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    device_results = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    if args.per_class_eval:\n",
    "        classes = val_loader.dataset.classes\n",
    "        confmat = ConfusionMatrix(len(classes))\n",
    "    else:\n",
    "        confmat = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (audio, target, _, devices) in enumerate(val_loader):\n",
    "            audio = audio.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "\n",
    "            # compute output\n",
    "            output, _ = model(audio)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            if confmat:\n",
    "                confmat.update(target, output.argmax(1))\n",
    "            losses.update(loss.item(), audio.size(0))\n",
    "            top1.update(acc1.item(), audio.size(0))\n",
    "            top5.update(acc5.item(), audio.size(0))\n",
    "\n",
    "            # Update device-specific results\n",
    "            pred = output.argmax(dim=1)\n",
    "            for j, dev in enumerate(devices):\n",
    "                device_results[dev]['total'] += 1\n",
    "                if pred[j] == target[j]:\n",
    "                    device_results[dev]['correct'] += 1\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "        if confmat:\n",
    "            print(confmat.format(classes))\n",
    "\n",
    "        print(\"\\nDevice-specific results:\")\n",
    "        for dev in device_results:\n",
    "            correct = device_results[dev]['correct']\n",
    "            total = device_results[dev]['total']\n",
    "            acc = 100.0 * correct / total\n",
    "            print(f'Device {dev}: {acc:.2f}% ({correct}/{total})')\n",
    "            \n",
    "    return top1.avg, device_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7544882-e87a-4578-8ddd-057bfc6642b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(root='./data/dcase', data='DCASE', source='source', target='target', temperature=2.0, alpha=1.9, trade_off=0.08, trade_off1=0.5, trade_off3=0.5, threshold=0.97, rho=0.5, batch_size=3, lr=0.0005, lr_gamma=0.001, lr_decay=0.75, momentum=0.9, weight_decay=0.001, workers=2, epochs=50, early=45, iters_per_epoch=1000, val_interval=10, print_freq=100, seed=None, per_class_eval=False, log='logs/dcase', phase='test', sample_rate=32000, clip_length=10, num_cls=10, device=device(type='cuda'))\n",
      "/home/teaching/miniconda3/lib/python3.12/site-packages/torch/functional.py:730: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /pytorch/aten/src/ATen/native/SpectralOps.cpp:875.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n",
      "/home/teaching/miniconda3/lib/python3.12/site-packages/hear21passt/models/preprocess.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "x torch.Size([3, 1, 128, 1000])\n",
      "/home/teaching/miniconda3/lib/python3.12/site-packages/hear21passt/models/passt.py:304: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "self.norm(x) torch.Size([3, 768, 12, 99])\n",
      " patch_embed :  torch.Size([3, 768, 12, 99])\n",
      " self.time_new_pos_embed.shape torch.Size([1, 768, 1, 99])\n",
      " self.freq_new_pos_embed.shape torch.Size([1, 768, 12, 1])\n",
      "X flattened torch.Size([3, 1188, 768])\n",
      " self.new_pos_embed.shape torch.Size([1, 2, 768])\n",
      " self.cls_tokens.shape torch.Size([3, 1, 768])\n",
      " self.dist_token.shape torch.Size([3, 1, 768])\n",
      " final sequence x torch.Size([3, 1190, 768])\n",
      " after 12 atten blocks x torch.Size([3, 1190, 768])\n",
      "forward_features torch.Size([3, 768])\n",
      "head torch.Size([3, 527])\n",
      "Test: [  0/110]\tTime  1.334 ( 1.334)\tLoss 7.9470e-01 (7.9470e-01)\tAcc@1  66.67 ( 66.67)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/110]\tTime  0.089 ( 0.101)\tLoss 1.7735e+00 (6.1348e-01)\tAcc@1  33.33 ( 76.90)\tAcc@5 100.00 (100.00)\n",
      " * Acc@1 76.970 Acc@5 100.000\n",
      "\n",
      "Device-specific results:\n",
      "Device a: 76.97% (254/330)\n",
      "76.96969576748934\n",
      "Test: [  0/880]\tTime  0.967 ( 0.967)\tLoss 6.3080e-01 (6.3080e-01)\tAcc@1  66.67 ( 66.67)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/880]\tTime  0.089 ( 0.098)\tLoss 1.2292e+00 (1.1568e+00)\tAcc@1  33.33 ( 56.77)\tAcc@5 100.00 ( 99.01)\n",
      "Test: [200/880]\tTime  0.090 ( 0.094)\tLoss 7.3186e-01 (1.1764e+00)\tAcc@1  66.67 ( 55.56)\tAcc@5 100.00 ( 98.18)\n",
      "Test: [300/880]\tTime  0.092 ( 0.093)\tLoss 5.6464e-01 (1.1813e+00)\tAcc@1 100.00 ( 55.37)\tAcc@5 100.00 ( 98.12)\n",
      "Test: [400/880]\tTime  0.092 ( 0.092)\tLoss 1.7552e+00 (1.1953e+00)\tAcc@1   0.00 ( 55.69)\tAcc@5 100.00 ( 97.84)\n",
      "Test: [500/880]\tTime  0.093 ( 0.093)\tLoss 2.3983e+00 (1.1824e+00)\tAcc@1  33.33 ( 56.42)\tAcc@5  66.67 ( 97.94)\n",
      "Test: [600/880]\tTime  0.093 ( 0.093)\tLoss 1.7106e+00 (1.1854e+00)\tAcc@1  33.33 ( 56.24)\tAcc@5 100.00 ( 97.78)\n",
      "Test: [700/880]\tTime  0.094 ( 0.093)\tLoss 2.1466e-01 (1.1812e+00)\tAcc@1 100.00 ( 56.44)\tAcc@5 100.00 ( 97.96)\n",
      "Test: [800/880]\tTime  0.095 ( 0.093)\tLoss 4.6872e-01 (1.1877e+00)\tAcc@1 100.00 ( 55.85)\tAcc@5 100.00 ( 98.13)\n",
      " * Acc@1 56.179 Acc@5 97.915\n",
      "\n",
      "Device-specific results:\n",
      "Device b: 62.61% (206/329)\n",
      "Device s3: 54.85% (181/330)\n",
      "Device s1: 46.36% (153/330)\n",
      "Device s2: 49.09% (162/330)\n",
      "Device c: 74.77% (246/329)\n",
      "Device s4: 53.03% (175/330)\n",
      "Device s6: 50.91% (168/330)\n",
      "Device s5: 57.88% (191/330)\n",
      "56.17892196487531\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d977c8-ce61-4f0e-a876-500a26c85352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
