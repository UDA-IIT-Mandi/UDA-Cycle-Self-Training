{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CST (Contrastive Self-Training) for SVHN and MNIST Datasets\n",
    "\n",
    "This notebook implements CST specifically for SVHN and MNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "# Import from local modules\n",
    "# Make sure these modules are in the same directory as the notebook or adjust the path accordingly\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from model.wrn import WideResNet\n",
    "from datasets.data import get_dataset, get_dataset_cst\n",
    "from utils.train_utils import get_args\n",
    "from utils.test_utils import AverageMeter, accuracy, setup_logger\n",
    "from utils.model_utils import init_weights, init_logging, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configuration and Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configuration parameters - you can modify these as needed\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Basic settings\n",
    "        self.seed = 1\n",
    "        self.out = './results'\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 64\n",
    "        self.lr = 0.001\n",
    "        self.wd = 5e-4\n",
    "        self.T = 1.0\n",
    "        self.alpha = 0.6\n",
    "        self.momentum = 0.9\n",
    "        self.ada_threshold = 0.9\n",
    "        self.lambda_cst = 1.0\n",
    "        self.lambda_emd = 0.0\n",
    "        self.lambda_rmc = 0.0\n",
    "        self.lambda_mmd = 0.0\n",
    "        self.lambda_coral = 0.0\n",
    "        self.k = 3\n",
    "        self.threshold = 0.95\n",
    "        self.temp = 0.1\n",
    "        self.epochs = 100\n",
    "        self.use_ema = True\n",
    "        self.ema_decay = 0.999\n",
    "        self.mu = 7\n",
    "        self.mix_mode = 'mixup'\n",
    "        self.mix_alpha = 0.1\n",
    "        self.eval_step = 1\n",
    "        self.total_steps = None\n",
    "        self.world_size = 1\n",
    "        self.rank = 0\n",
    "        \n",
    "        # Dataset settings - focusing only on SVHN and MNIST\n",
    "        self.num_classes = 10\n",
    "        self.dataset = 'digit'\n",
    "        self.src_dataset = None  # We'll set this when needed\n",
    "        self.trg_dataset = None  # We'll set this when needed\n",
    "        self.arch = 'wrn'\n",
    "        self.num_workers = 2\n",
    "        self.expand_labels = True\n",
    "        self.data_path = './data'\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device and random seed for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the EMA Class for Model Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CST Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cst_loss_func(pred_st, pred_w):\n",
    "    pred_st = F.softmax(pred_st, dim=1)\n",
    "    pred_w = F.softmax(pred_w, dim=1)\n",
    "    pred_w = pred_w.detach()\n",
    "    \n",
    "    # Calculate mean square error loss\n",
    "    loss = torch.mean((pred_st - pred_w) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Create WideResNet model for digit classification\n",
    "    model = WideResNet(num_classes=10)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Initialize model weights\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, ema_model, labeled_trainloader, unlabeled_trainloader, optimizer, scheduler, epoch, args):\n",
    "    model.train()\n",
    "    \n",
    "    # Set up metrics tracking\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_cst = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    labeled_iter = iter(labeled_trainloader)\n",
    "    unlabeled_iter = iter(unlabeled_trainloader)\n",
    "    \n",
    "    # Number of batches = min(len(labeled_trainloader), len(unlabeled_trainloader))\n",
    "    num_batches = min(len(labeled_trainloader), len(unlabeled_trainloader))\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        try:\n",
    "            inputs_x, targets_x = next(labeled_iter)\n",
    "        except StopIteration:\n",
    "            labeled_iter = iter(labeled_trainloader)\n",
    "            inputs_x, targets_x = next(labeled_iter)\n",
    "            \n",
    "        try:\n",
    "            (inputs_u_w, inputs_u_s), _ = next(unlabeled_iter)\n",
    "        except StopIteration:\n",
    "            unlabeled_iter = iter(unlabeled_trainloader)\n",
    "            (inputs_u_w, inputs_u_s), _ = next(unlabeled_iter)\n",
    "            \n",
    "        data_time.update(time.time() - end)\n",
    "        batch_size = inputs_x.shape[0]\n",
    "        \n",
    "        # Move tensors to device\n",
    "        inputs_x = inputs_x.to(device)\n",
    "        targets_x = targets_x.to(device)\n",
    "        inputs_u_w = inputs_u_w.to(device)\n",
    "        inputs_u_s = inputs_u_s.to(device)\n",
    "        \n",
    "        # Forward pass for labeled data\n",
    "        logits_x = model(inputs_x)\n",
    "        loss_x = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
    "        \n",
    "        # Forward pass for unlabeled data\n",
    "        with torch.no_grad():\n",
    "            logits_u_w = model(inputs_u_w)\n",
    "        \n",
    "        logits_u_s = model(inputs_u_s)\n",
    "        \n",
    "        # Calculate CST loss\n",
    "        loss_cst = cst_loss_func(logits_u_s, logits_u_w)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_x + args.lambda_cst * loss_cst\n",
    "        \n",
    "        # Update metrics\n",
    "        losses.update(loss.item())\n",
    "        losses_x.update(loss_x.item())\n",
    "        losses_cst.update(loss_cst.item())\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update EMA model\n",
    "        if args.use_ema:\n",
    "            ema_model.update()\n",
    "            \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} | Batch: {batch_idx + 1}/{num_batches} | \"  \n",
    "                  f\"Loss: {losses.avg:.4f} | Loss_x: {losses_x.avg:.4f} | Loss_cst: {losses_cst.avg:.4f} | \"\n",
    "                  f\"Time: {batch_time.avg:.4f}s | Data: {data_time.avg:.4f}s\")\n",
    "            \n",
    "    return losses.avg, losses_x.avg, losses_cst.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline for SVHN to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svhn_to_mnist():\n",
    "    # Set source and target datasets\n",
    "    args.src_dataset = 'svhn'\n",
    "    args.trg_dataset = 'mnist'\n",
    "    \n",
    "    # Create output directory\n",
    "    if not os.path.exists(args.out):\n",
    "        os.makedirs(args.out)\n",
    "    \n",
    "    # Get datasets\n",
    "    labeled_dataset, unlabeled_dataset, test_dataset = get_dataset_cst(\n",
    "        args, args.src_dataset, args.trg_dataset)\n",
    "    \n",
    "    # Create data loaders\n",
    "    labeled_trainloader = DataLoader(\n",
    "        labeled_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "    \n",
    "    unlabeled_trainloader = DataLoader(\n",
    "        unlabeled_dataset,\n",
    "        batch_size=args.batch_size * args.mu,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Create EMA model\n",
    "    ema_model = None\n",
    "    if args.use_ema:\n",
    "        ema_model = EMA(model, args.ema_decay)\n",
    "        ema_model.register()\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.wd)\n",
    "    \n",
    "    # Set total steps\n",
    "    args.total_steps = args.epochs * min(len(labeled_trainloader), len(unlabeled_trainloader))\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, 0, args.total_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    best_acc = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train for one epoch\n",
    "        train_loss, train_loss_x, train_loss_cst = train_step(\n",
    "            model, ema_model, labeled_trainloader, unlabeled_trainloader, optimizer, scheduler, epoch, args)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        if args.use_ema:\n",
    "            ema_model.apply_shadow()\n",
    "            test_acc = validate(model, test_loader)\n",
    "            ema_model.restore()\n",
    "        else:\n",
    "            test_acc = validate(model, test_loader)\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            if args.use_ema:\n",
    "                ema_model.apply_shadow()\n",
    "                torch.save(model.state_dict(), os.path.join(args.out, 'best_model.pt'))\n",
    "                ema_model.restore()\n",
    "            else:\n",
    "                torch.save(model.state_dict(), os.path.join(args.out, 'best_model.pt'))\n",
    "        \n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Test Acc: {test_acc:.4f} | Best Acc: {best_acc:.4f}\")\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline for MNIST to SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_to_svhn():\n",
    "    # Set source and target datasets\n",
    "    args.src_dataset = 'mnist'\n",
    "    args.trg_dataset = 'svhn'\n",
    "    \n",
    "    # Create output directory\n",
    "    if not os.path.exists(args.out):\n",
    "        os.makedirs(args.out)\n",
    "    \n",
    "    # Get datasets\n",
    "    labeled_dataset, unlabeled_dataset, test_dataset = get_dataset_cst(\n",
    "        args, args.src_dataset, args.trg_dataset)\n",
    "    \n",
    "    # Create data loaders\n",
    "    labeled_trainloader = DataLoader(\n",
    "        labeled_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "    \n",
    "    unlabeled_trainloader = DataLoader(\n",
    "        unlabeled_dataset,\n",
    "        batch_size=args.batch_size * args.mu,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Create EMA model\n",
    "    ema_model = None\n",
    "    if args.use_ema:\n",
    "        ema_model = EMA(model, args.ema_decay)\n",
    "        ema_model.register()\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.wd)\n",
    "    \n",
    "    # Set total steps\n",
    "    args.total_steps = args.epochs * min(len(labeled_trainloader), len(unlabeled_trainloader))\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, 0, args.total_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    best_acc = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train for one epoch\n",
    "        train_loss, train_loss_x, train_loss_cst = train_step(\n",
    "            model, ema_model, labeled_trainloader, unlabeled_trainloader, optimizer, scheduler, epoch, args)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        if args.use_ema:\n",
    "            ema_model.apply_shadow()\n",
    "            test_acc = validate(model, test_loader)\n",
    "            ema_model.restore()\n",
    "        else:\n",
    "            test_acc = validate(model, test_loader)\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            if args.use_ema:\n",
    "                ema_model.apply_shadow()\n",
    "                torch.save(model.state_dict(), os.path.join(args.out, 'best_model.pt'))\n",
    "                ema_model.restore()\n",
    "            else:\n",
    "                torch.save(model.state_dict(), os.path.join(args.out, 'best_model.pt'))\n",
    "        \n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Test Acc: {test_acc:.4f} | Best Acc: {best_acc:.4f}\")\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which experiment to run\n",
    "experiment = \"svhn_to_mnist\"  # or \"mnist_to_svhn\"\n",
    "\n",
    "if experiment == \"svhn_to_mnist\":\n",
    "    print(\"Running SVHN to MNIST experiment...\")\n",
    "    best_acc = train_svhn_to_mnist()\n",
    "    print(f\"Best accuracy for SVHN to MNIST: {best_acc:.4f}\")\n",
    "else:\n",
    "    print(\"Running MNIST to SVHN experiment...\")\n",
    "    best_acc = train_mnist_to_svhn()\n",
    "    print(f\"Best accuracy for MNIST to SVHN: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Add code for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_samples(dataset_name):\n",
    "    # Load the dataset\n",
    "    if dataset_name == 'svhn':\n",
    "        dataset = get_dataset(args, 'svhn')\n",
    "    else:\n",
    "        dataset = get_dataset(args, 'mnist')\n",
    "    \n",
    "    # Plot some samples\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(10):\n",
    "        img, label = dataset[i]\n",
    "        img = img.numpy().transpose(1, 2, 0)  # Convert to H x W x C format\n",
    "        \n",
    "        # Handle grayscale images\n",
    "        if img.shape[2] == 1:\n",
    "            img = img.squeeze()\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{dataset_name.upper()} Dataset Samples\", y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVHN samples\n",
    "visualize_samples('svhn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MNIST samples\n",
    "visualize_samples('mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}