{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb358fe-3242-47d0-a24a-1a6aadc47142",
   "metadata": {},
   "source": [
    "# Cycle Self Training Implementation on DCASE TAU 2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e86ed20-4362-4f67-80ef-33cf50eddd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import argparse\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import librosa\n",
    "from hear21passt.base import get_basic_model\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8ff3da-dd18-4d39-8bdf-5aff96709d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import common.audio.datasets as datasets\n",
    "from common.audio.transforms import TransformFixMatch\n",
    "from common.audio.transforms import WeakAugment\n",
    "from common.audio.transforms import StrongAugment\n",
    "from common.utils.data import ForeverDataIterator\n",
    "from common.utils.metric import accuracy, ConfusionMatrix\n",
    "from common.utils.meter import AverageMeter, ProgressMeter\n",
    "from common.utils.logger import CompleteLogger\n",
    "from common.utils.analysis import collect_feature, tsne, a_distance\n",
    "from common.tools.randaugment import rand_augment_transform, GaussianBlur\n",
    "from common.tools.sam import SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902e6538-8731-4ca0-a428-1d9532f64388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4c41f7-5bb7-4a4e-9fc5-2dfbb9b277a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments set for CST experiment (DCASE TAU 2020):\n",
      "root = ./data/dcase\n",
      "data = DCASE\n",
      "source = source\n",
      "target = target\n",
      "temperature = 2.0\n",
      "alpha = 1.9\n",
      "trade_off = 0.08\n",
      "trade_off1 = 0.5\n",
      "trade_off3 = 0.5\n",
      "threshold = 0.97\n",
      "rho = 0.5\n",
      "batch_size = 2\n",
      "lr = 0.001\n",
      "lr_gamma = 0.001\n",
      "lr_decay = 0.75\n",
      "momentum = 0.9\n",
      "weight_decay = 0.001\n",
      "workers = 8\n",
      "epochs = 60\n",
      "early = 50\n",
      "iters_per_epoch = 4000\n",
      "val_interval = 5\n",
      "print_freq = 100\n",
      "seed = None\n",
      "per_class_eval = False\n",
      "log = logs/dcase\n",
      "phase = train\n",
      "sample_rate = 32000\n",
      "clip_length = 10\n",
      "num_cls = 10\n",
      "device = cuda\n",
      "gradient_accumulation_steps = 16\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    root = './data/dcase',\n",
    "    data = 'DCASE',          \n",
    "    source = 'source',      \n",
    "    target = 'target',\n",
    "    temperature = 2.0,\n",
    "    alpha = 1.9,\n",
    "    trade_off = 0.08,\n",
    "    trade_off1 = 0.5,\n",
    "    trade_off3 = 0.5,\n",
    "    threshold = 0.97,\n",
    "    rho = 0.5,\n",
    "    batch_size = 2,\n",
    "    lr = 0.001,\n",
    "    lr_gamma = 0.001,\n",
    "    lr_decay = 0.75,\n",
    "    momentum = 0.9,\n",
    "    weight_decay = 1e-3,\n",
    "    workers = 8,\n",
    "    epochs = 60,\n",
    "    early = 50,\n",
    "    iters_per_epoch = 4000,\n",
    "    val_interval = 5,\n",
    "    print_freq = 100,        \n",
    "    seed = None,             \n",
    "    per_class_eval = False,  \n",
    "    log = 'logs/dcase',             \n",
    "    phase = 'train', \n",
    "    sample_rate = 32000,\n",
    "    clip_length = 10,\n",
    "    num_cls = 10,\n",
    "    device = device,\n",
    "    gradient_accumulation_steps = 16,\n",
    "    \n",
    ")\n",
    "print(\"Arguments set for CST experiment (DCASE TAU 2020):\")\n",
    "for k, v in args.__dict__.items():\n",
    "    print(f\"{k} = {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ca24ac-0755-4100-85e6-72bfb953f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(predictions: torch.Tensor, reduction='none') -> torch.Tensor:\n",
    "    epsilon = 1e-5\n",
    "    H = -predictions * torch.log(predictions + epsilon)\n",
    "    H = H.sum(dim=1)\n",
    "    if reduction == 'mean':\n",
    "        return H.mean()\n",
    "    else:\n",
    "        return H\n",
    "\n",
    "class TsallisEntropy(nn.Module):\n",
    "    def __init__(self, temperature: float, alpha: float):\n",
    "        super(TsallisEntropy, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        N, C = logits.shape\n",
    "        pred = F.softmax(logits / self.temperature, dim=1) \n",
    "        entropy_weight = entropy(pred).detach()\n",
    "        entropy_weight = 1 + torch.exp(-entropy_weight)\n",
    "        entropy_weight = (N * entropy_weight / torch.sum(entropy_weight)).unsqueeze(dim=1)  \n",
    "        sum_dim = torch.sum(pred * entropy_weight, dim = 0).unsqueeze(dim=0)\n",
    "        return 1 / (self.alpha - 1) * torch.sum((1 / torch.mean(sum_dim) - torch.sum(pred ** self.alpha / sum_dim * entropy_weight, dim = -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc43990-595a-40de-a9b8-28c12a583d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier (nn.Module):\n",
    "    def __init__(self, backbone, num_classes, bottleneck_dim=256, finetune=True):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.backbone = backbone  \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(768, bottleneck_dim),  \n",
    "            nn.BatchNorm1d(bottleneck_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.head = nn.Linear(bottleneck_dim, num_classes)\n",
    "        self.finetune = finetune\n",
    "\n",
    "        if not finetune:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.bottleneck(features)\n",
    "        outputs = self.head(embeddings)\n",
    "        return outputs, embeddings\n",
    "\n",
    "    def get_parameters(self, base_lr=1.0) -> List[Dict]:\n",
    "        \"\"\"A parameter list which decides optimization hyper-parameters,\n",
    "            such as the relative learning rate of each layer\n",
    "        \"\"\"\n",
    "        params = [\n",
    "            {\"params\": self.backbone.parameters(), \"lr\": 0.1 * base_lr if self.finetune else 1.0 * base_lr},\n",
    "            {\"params\": self.bottleneck.parameters(), \"lr\": 1.0 * base_lr},\n",
    "            {\"params\": self.head.parameters(), \"lr\": 1.0 * base_lr},\n",
    "        ]\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0d3ca3-e406-493a-9153-f96742d3156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train source samples: 9193\n",
      "Train target samples: 3372\n",
      "Val source samples: 1022\n",
      "Val target samples: 375\n",
      "Test source samples: 330\n",
      "Test target samples: 2638\n"
     ]
    }
   ],
   "source": [
    "# Data loading code\n",
    "train_transform = WeakAugment()\n",
    "unlabeled_transform = TransformFixMatch()\n",
    "val_transform = WeakAugment()  \n",
    "\n",
    "dataset = datasets.__dict__[args.data]\n",
    "\n",
    "base_source_dataset = dataset(root=args.root, task=args.source, split='train', transform=None)\n",
    "base_target_dataset = dataset(root=args.root, task=args.target, split='train', transform=None)\n",
    "\n",
    "train_size_s = int(0.9 * len(base_source_dataset))\n",
    "val_size_s = len(base_source_dataset) - train_size_s\n",
    "train_indices_s, val_indices_s = torch.utils.data.random_split(range(len(base_source_dataset)), [train_size_s, val_size_s])\n",
    "\n",
    "train_size_t = int(0.9 * len(base_target_dataset))\n",
    "val_size_t = len(base_target_dataset) - train_size_t\n",
    "train_indices_t, val_indices_t = torch.utils.data.random_split(range(len(base_target_dataset)), [train_size_t, val_size_t])\n",
    "\n",
    "train_source_dataset = torch.utils.data.Subset(\n",
    "    dataset(root=args.root, task=args.source, split='train', transform=train_transform),\n",
    "    train_indices_s.indices\n",
    ")\n",
    "\n",
    "val_source_dataset = torch.utils.data.Subset(\n",
    "    dataset(root=args.root, task=args.source, split='train', transform=val_transform),\n",
    "    val_indices_s.indices\n",
    ")\n",
    "\n",
    "train_target_dataset = torch.utils.data.Subset(\n",
    "    dataset(root=args.root, task=args.target, split='train', transform=unlabeled_transform),\n",
    "    train_indices_t.indices\n",
    ")\n",
    "\n",
    "val_target_dataset = torch.utils.data.Subset(\n",
    "    dataset(root=args.root, task=args.target, split='train', transform=val_transform),\n",
    "    val_indices_t.indices\n",
    ")\n",
    "\n",
    "train_source_loader = DataLoader(train_source_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last=True)\n",
    "train_target_loader = DataLoader(train_target_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last=True)\n",
    "val_source_loader = DataLoader(val_source_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "val_target_loader = DataLoader(val_target_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "test_source_dataset = dataset(root=args.root, task=args.source, split='test', transform=val_transform)\n",
    "test_source_loader = DataLoader(test_source_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "test_target_dataset = dataset(root=args.root, task=args.target, split='test', transform=val_transform)\n",
    "test_target_loader = DataLoader(test_target_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "\n",
    "train_source_iter = ForeverDataIterator(train_source_loader)\n",
    "train_target_iter = ForeverDataIterator(train_target_loader)\n",
    "\n",
    "print(f\"Train source samples: {len(train_source_dataset)}\")\n",
    "print(f\"Train target samples: {len(train_target_dataset)}\")\n",
    "print(f\"Val source samples: {len(val_source_dataset)}\")\n",
    "print(f\"Val target samples: {len(val_target_dataset)}\")\n",
    "print(f\"Test source samples: {len(test_source_dataset)}\")\n",
    "print(f\"Test target samples: {len(test_target_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2517c4e8-8a77-4814-b01a-5052d8af937b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model PaSST\n",
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "print(\"=> using pre-trained model PaSST\")\n",
    "backbone = get_basic_model(mode=\"embed_only\")\n",
    "backbone.eval() \n",
    "classifier = AudioClassifier(backbone, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a493046-e2f5-4d9b-9fe9-561a630b2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args: argparse.Namespace):\n",
    "    logger = CompleteLogger(args.log, args.phase)\n",
    "    print(args)\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    # define optimizer and lr scheduler\n",
    "    base_optimizer = SGD\n",
    "    optimizer = SAM(classifier.get_parameters(), base_optimizer, lr = args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay, adaptive = True, rho = args.rho)\n",
    "    lr_scheduler = LambdaLR(optimizer, lambda x:  args.lr * (1. + args.lr_gamma * float(x)) ** (-args.lr_decay))\n",
    "\n",
    "    # define loss function\n",
    "    ts_loss = TsallisEntropy(temperature=args.temperature, alpha = args.alpha)\n",
    "\n",
    "    # resume from the best checkpoint\n",
    "    if args.phase != 'train':\n",
    "        checkpoint = torch.load(logger.get_checkpoint_path('best'), map_location='cpu')\n",
    "        classifier.load_state_dict(checkpoint)\n",
    "\n",
    "    # analysis the model\n",
    "    if args.phase == 'analysis':\n",
    "        # extract features from both domains\n",
    "        feature_extractor = nn.Sequential(classifier.backbone, classifier.bottleneck).to(device)\n",
    "        source_feature = collect_feature(train_source_loader, feature_extractor, device)\n",
    "        target_feature = collect_feature(train_target_loader, feature_extractor, device)\n",
    "        # plot t-SNE\n",
    "        tSNE_filename = osp.join(logger.visualize_directory, 'TSNE.png')\n",
    "        tsne.visualize(source_feature, target_feature, tSNE_filename)\n",
    "        print(\"Saving t-SNE to\", tSNE_filename)\n",
    "        # calculate A-distance, which is a measure for distribution discrepancy\n",
    "        A_distance = a_distance.calculate(source_feature, target_feature, device)\n",
    "        print(\"A-distance =\", A_distance)\n",
    "        return\n",
    "\n",
    "    if args.phase == 'test':\n",
    "        acc_s, _ = validate(test_source_loader, classifier, args)\n",
    "        print(acc_s)\n",
    "\n",
    "        acc_t, _ = validate(test_target_loader, classifier, args)\n",
    "        print(acc_t)\n",
    "        return\n",
    "\n",
    "    # start training\n",
    "    best_acc_s = 0.\n",
    "    best_acc_t = 0.\n",
    "    for epoch in range(min(args.epochs, args.early)):\n",
    "        print(\"lr:\", lr_scheduler.get_last_lr()[0])\n",
    "        # train for one epoch\n",
    "        train(train_source_iter, train_target_iter, classifier, ts_loss, optimizer,\n",
    "              lr_scheduler, epoch, args)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        if (epoch + 1) % args.val_interval == 0 or epoch == args.epochs - 1:\n",
    "            acc_s, device_results_s = validate(val_source_loader, classifier, args)\n",
    "            acc_t, device_results_t = validate(val_target_loader, classifier, args)\n",
    "            \n",
    "    \n",
    "            # remember best acc@1 and save checkpoint\n",
    "            torch.save(classifier.state_dict(), logger.get_checkpoint_path('latest'))\n",
    "            if acc_s > best_acc_s:\n",
    "                shutil.copy(logger.get_checkpoint_path('latest'), logger.get_checkpoint_path('best'))\n",
    "            best_acc_s = max(acc_s, best_acc_s)\n",
    "    \n",
    "            if acc_t > best_acc_t:\n",
    "                shutil.copy(logger.get_checkpoint_path('latest'), logger.get_checkpoint_path('best'))\n",
    "            best_acc_t = max(acc_t, best_acc_t)\n",
    "\n",
    "            print(\"Best Source Accuracy = {:3.1f}\".format(best_acc_s))\n",
    "            print(\"Best Target Accuracy = {:3.1f}\".format(best_acc_t))\n",
    "\n",
    "        else:\n",
    "            torch.save(classifier.state_dict(), logger.get_checkpoint_path('latest'))\n",
    "\n",
    "    \n",
    "\n",
    "    # evaluate on test set\n",
    "    classifier.load_state_dict(torch.load(logger.get_checkpoint_path('best')))\n",
    "    acc_s, _ = validate(test_source_loader, classifier, args)\n",
    "    print(\"Test Source Accuracy = {:3.1f}\".format(acc_s))\n",
    "\n",
    "    acc_t, _ = validate(test_target_loader, classifier, args)\n",
    "    print(\"Test Target Accuracy = {:3.1f}\".format(acc_t))\n",
    "\n",
    "    logger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee50557-88e9-47f2-89e9-2d3bb90d2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_source_iter: ForeverDataIterator, train_target_iter: ForeverDataIterator,\n",
    "          model: AudioClassifier, ts: TsallisEntropy, optimizer: SGD,\n",
    "          lr_scheduler: LambdaLR, epoch: int, args: argparse.Namespace):\n",
    "    batch_time = AverageMeter('Time', ':3.1f')\n",
    "    data_time = AverageMeter('Data', ':3.1f')\n",
    "    losses = AverageMeter('Loss', ':3.2f')\n",
    "    trans_losses = AverageMeter('Trans Loss', ':3.2f')\n",
    "    rev_losses = AverageMeter('CST Loss', ':3.2f')\n",
    "    fix_losses = AverageMeter('Fix Loss', ':3.2f')\n",
    "    cls_accs = AverageMeter('Cls Acc', ':3.1f')\n",
    "\n",
    "    progress = ProgressMeter(\n",
    "        args.iters_per_epoch,\n",
    "        [batch_time, data_time, losses, trans_losses, rev_losses, fix_losses, cls_accs],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    accumulated_loss = 0.0  # Track accumulated loss\n",
    "    \n",
    "    for i in range(args.iters_per_epoch):\n",
    "        x_s, labels_s, _, _ = next(train_source_iter)\n",
    "        (x_t, x_t_u), _, _, _ = next(train_target_iter)\n",
    "\n",
    "        x_s = x_s.to(device)\n",
    "        x_t = x_t.to(device)\n",
    "        x_t_u = x_t_u.to(device)\n",
    "        labels_s = labels_s.to(device)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # compute output\n",
    "        x = torch.cat((x_s, x_t), dim=0)\n",
    "        y, f = model(x)\n",
    "        y_t_u, _ = model(x_t_u)\n",
    "\n",
    "        f_s, f_t = f.chunk(2, dim=0)\n",
    "        y_s, y_t = y.chunk(2, dim=0)\n",
    "\n",
    "        # generate target pseudo-labels\n",
    "        max_prob, pred_u = torch.max(F.softmax(y_t, dim=-1), dim=-1)\n",
    "        Lu = (F.cross_entropy(y_t_u, pred_u,\n",
    "                              reduction='none') * max_prob.ge(args.threshold).float().detach()).mean()\n",
    "\n",
    "        # compute cst\n",
    "        target_data_train_r = f_t\n",
    "        target_data_train_r = target_data_train_r / (torch.norm(target_data_train_r, dim = -1).reshape(target_data_train_r.shape[0], 1))\n",
    "        target_data_test_r = f_s\n",
    "        target_data_test_r = target_data_test_r / (torch.norm(target_data_test_r, dim = -1).reshape(target_data_test_r.shape[0], 1))\n",
    "        target_gram_r = torch.clamp(target_data_train_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "        target_kernel_r = target_gram_r\n",
    "        test_gram_r = torch.clamp(target_data_test_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "        test_kernel_r = test_gram_r\n",
    "        target_train_label_r = torch.nn.functional.one_hot(pred_u, args.num_cls) - 1 / float(args.num_cls) \n",
    "        target_test_pred_r = test_kernel_r.mm(torch.inverse(target_kernel_r + 0.001 * torch.eye(args.batch_size).cuda())).mm(target_train_label_r)\n",
    "        reverse_loss = nn.MSELoss()(target_test_pred_r, torch.nn.functional.one_hot(labels_s, args.num_cls) - 1 / float(args.num_cls)) \n",
    "\n",
    "        cls_loss = F.cross_entropy(y_s, labels_s)\n",
    "        transfer_loss = ts(y_t)\n",
    "\n",
    "        if Lu != 0:\n",
    "            loss = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1 + Lu * args.trade_off3\n",
    "        else: \n",
    "            loss = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1\n",
    "\n",
    "        # Scale loss by accumulation steps\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        accumulated_loss += loss.item()\n",
    "\n",
    "        cls_acc = accuracy(y_s, labels_s)[0]\n",
    "\n",
    "        # Update meters with original (unscaled) loss values for logging\n",
    "        losses.update(loss.item() * args.gradient_accumulation_steps, x_s.size(0))\n",
    "        cls_accs.update(cls_acc.item(), x_s.size(0))\n",
    "        trans_losses.update(transfer_loss.item(), x_s.size(0))\n",
    "        rev_losses.update(reverse_loss.item(), x_s.size(0))\n",
    "        fix_losses.update(Lu.item(), x_s.size(0))\n",
    "\n",
    "        # compute gradient and do the first SAM step (accumulate gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Only perform optimizer steps every gradient_accumulation_steps\n",
    "        if (i + 1) % args.gradient_accumulation_steps == 0:\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # compute gradient and do the second SAM step\n",
    "            # Re-compute forward pass for SAM second step\n",
    "            x = torch.cat((x_s, x_t), dim=0)\n",
    "            y, f = model(x)\n",
    "            y_t_u, _ = model(x_t_u)\n",
    "\n",
    "            f_s, f_t = f.chunk(2, dim=0)\n",
    "            y_s, y_t = y.chunk(2, dim=0)\n",
    "\n",
    "            # generate target pseudo-labels\n",
    "            max_prob, pred_u = torch.max(F.softmax(y_t, dim=-1), dim=-1)\n",
    "            Lu = (F.cross_entropy(y_t_u, pred_u,\n",
    "                                  reduction='none') * max_prob.ge(args.threshold).float().detach()).mean()\n",
    "\n",
    "            # compute cst\n",
    "            target_data_train_r = f_t\n",
    "            target_data_train_r = target_data_train_r / (torch.norm(target_data_train_r, dim = -1).reshape(target_data_train_r.shape[0], 1))\n",
    "            target_data_test_r = f_s\n",
    "            target_data_test_r = target_data_test_r / (torch.norm(target_data_test_r, dim = -1).reshape(target_data_test_r.shape[0], 1))\n",
    "            target_gram_r = torch.clamp(target_data_train_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "            target_kernel_r = target_gram_r\n",
    "            test_gram_r = torch.clamp(target_data_test_r.mm(target_data_train_r.transpose(dim0 = 1, dim1 = 0)),-0.99999999,0.99999999)\n",
    "            test_kernel_r = test_gram_r\n",
    "            target_train_label_r = torch.nn.functional.one_hot(pred_u, args.num_cls) - 1 / float(args.num_cls) \n",
    "            target_test_pred_r = test_kernel_r.mm(torch.inverse(target_kernel_r + 0.001 * torch.eye(args.batch_size).cuda())).mm(target_train_label_r)\n",
    "            reverse_loss = nn.MSELoss()(target_test_pred_r, torch.nn.functional.one_hot(labels_s, args.num_cls) - 1 / float(args.num_cls)) \n",
    "\n",
    "            cls_loss = F.cross_entropy(y_s, labels_s)\n",
    "            transfer_loss = ts(y_t)\n",
    "\n",
    "            if Lu != 0:\n",
    "                loss1 = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1 + Lu * args.trade_off3\n",
    "            else: \n",
    "                loss1 = cls_loss + transfer_loss * args.trade_off + reverse_loss * args.trade_off1\n",
    "\n",
    "            # Scale the second loss as well\n",
    "            loss1 = loss1 / args.gradient_accumulation_steps\n",
    "            loss1.backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "            \n",
    "            # Reset accumulated loss\n",
    "            accumulated_loss = 0.0\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92762d86-1804-45cd-ba17-a2df7fd42bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_device(model, test_target_loader, device):\n",
    "    model.eval()\n",
    "    device_results = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for audio, labels, domains, devices in test_target_loader:\n",
    "            audio = audio.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(audio)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            for i, dev in enumerate(devices):\n",
    "                device_results[dev]['total'] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    device_results[dev]['correct'] += 1\n",
    "    \n",
    "    results = {}\n",
    "    for dev in device_results:\n",
    "        correct = device_results[dev]['correct']\n",
    "        total = device_results[dev]['total']\n",
    "        accuracy = 100.0 * correct / total\n",
    "        results[dev] = {'accuracy': accuracy, 'total': total}\n",
    "        print(f'Device {dev}: Accuracy = {accuracy:.2f}% ({correct}/{total})')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df9e46d-97d8-42d7-84a0-2d6b5f284477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader: DataLoader, model: AudioClassifier, args: argparse.Namespace) -> float:\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    device_results = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    if args.per_class_eval:\n",
    "        classes = val_loader.dataset.classes\n",
    "        confmat = ConfusionMatrix(len(classes))\n",
    "    else:\n",
    "        confmat = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (audio, target, _, devices) in enumerate(val_loader):\n",
    "            audio = audio.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "\n",
    "            # compute output\n",
    "            output, _ = model(audio)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            if confmat:\n",
    "                confmat.update(target, output.argmax(1))\n",
    "            losses.update(loss.item(), audio.size(0))\n",
    "            top1.update(acc1.item(), audio.size(0))\n",
    "            top5.update(acc5.item(), audio.size(0))\n",
    "\n",
    "            # Update device-specific results\n",
    "            pred = output.argmax(dim=1)\n",
    "            for j, dev in enumerate(devices):\n",
    "                device_results[dev]['total'] += 1\n",
    "                if pred[j] == target[j]:\n",
    "                    device_results[dev]['correct'] += 1\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "        if confmat:\n",
    "            print(confmat.format(classes))\n",
    "\n",
    "        print(\"\\nDevice-specific results:\")\n",
    "        for dev in device_results:\n",
    "            correct = device_results[dev]['correct']\n",
    "            total = device_results[dev]['total']\n",
    "            acc = 100.0 * correct / total\n",
    "            print(f'Device {dev}: {acc:.2f}% ({correct}/{total})')\n",
    "            \n",
    "    return top1.avg, device_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7544882-e87a-4578-8ddd-057bfc6642b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(root='./data/dcase', data='DCASE', source='source', target='target', temperature=2.0, alpha=1.9, trade_off=0.08, trade_off1=0.5, trade_off3=0.5, threshold=0.97, rho=0.5, batch_size=2, lr=0.001, lr_gamma=0.001, lr_decay=0.75, momentum=0.9, weight_decay=0.001, workers=8, epochs=60, early=50, iters_per_epoch=4000, val_interval=5, print_freq=100, seed=None, per_class_eval=False, log='logs/dcase', phase='train', sample_rate=32000, clip_length=10, num_cls=10, device=device(type='cuda'), gradient_accumulation_steps=16)\n",
      "lr: 0.0001\n",
      "/home/teaching/miniconda3/lib/python3.12/site-packages/torch/functional.py:730: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /pytorch/aten/src/ATen/native/SpectralOps.cpp:875.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n",
      "/home/teaching/miniconda3/lib/python3.12/site-packages/hear21passt/models/preprocess.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "x torch.Size([4, 1, 128, 1000])\n",
      "/home/teaching/miniconda3/lib/python3.12/site-packages/hear21passt/models/passt.py:304: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "self.norm(x) torch.Size([4, 768, 12, 99])\n",
      " patch_embed :  torch.Size([4, 768, 12, 99])\n",
      " self.time_new_pos_embed.shape torch.Size([1, 768, 1, 99])\n",
      " self.freq_new_pos_embed.shape torch.Size([1, 768, 12, 1])\n",
      "X flattened torch.Size([4, 1188, 768])\n",
      " self.new_pos_embed.shape torch.Size([1, 2, 768])\n",
      " self.cls_tokens.shape torch.Size([4, 1, 768])\n",
      " self.dist_token.shape torch.Size([4, 1, 768])\n",
      " final sequence x torch.Size([4, 1190, 768])\n",
      " after 12 atten blocks x torch.Size([4, 1190, 768])\n",
      "forward_features torch.Size([4, 768])\n",
      "head torch.Size([4, 527])\n",
      "Epoch: [0][   0/4000]\tTime 1.1 (1.1)\tData 0.0 (0.0)\tLoss 3.83 (3.83)\tTrans Loss 9.69 (9.69)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (0.0)\n",
      "/home/teaching/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch: [0][ 100/4000]\tTime 0.5 (0.5)\tData 0.2 (0.2)\tLoss 3.07 (3.22)\tTrans Loss 9.70 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (8.4)\n",
      "Epoch: [0][ 200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.45 (3.24)\tTrans Loss 9.67 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (7.5)\n",
      "Epoch: [0][ 300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.76 (3.22)\tTrans Loss 9.68 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (8.5)\n",
      "Epoch: [0][ 400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.24 (3.22)\tTrans Loss 9.69 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (8.9)\n",
      "Epoch: [0][ 500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.46 (3.21)\tTrans Loss 9.69 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (9.1)\n",
      "Epoch: [0][ 600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.20 (3.20)\tTrans Loss 9.71 (9.70)\tCST Loss 0.09 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (9.4)\n",
      "Epoch: [0][ 700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.38 (3.19)\tTrans Loss 9.68 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (9.7)\n",
      "Epoch: [0][ 800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.40 (3.19)\tTrans Loss 9.70 (9.70)\tCST Loss 0.11 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (9.9)\n",
      "Epoch: [0][ 900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.47 (3.18)\tTrans Loss 9.70 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (10.4)\n",
      "Epoch: [0][1000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.97 (3.18)\tTrans Loss 9.68 (9.70)\tCST Loss 0.08 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (10.8)\n",
      "Epoch: [0][1100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.41 (3.17)\tTrans Loss 9.70 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (11.2)\n",
      "Epoch: [0][1200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.76 (3.16)\tTrans Loss 9.69 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (12.0)\n",
      "Epoch: [0][1300/4000]\tTime 0.5 (0.5)\tData 0.2 (0.2)\tLoss 3.51 (3.16)\tTrans Loss 9.70 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (12.4)\n",
      "Epoch: [0][1400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.42 (3.15)\tTrans Loss 9.70 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (13.0)\n",
      "Epoch: [0][1500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.75 (3.14)\tTrans Loss 9.68 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (13.9)\n",
      "Epoch: [0][1600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.02 (3.14)\tTrans Loss 9.69 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (14.1)\n",
      "Epoch: [0][1700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.89 (3.13)\tTrans Loss 9.69 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (14.5)\n",
      "Epoch: [0][1800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.36 (3.12)\tTrans Loss 9.70 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (14.8)\n",
      "Epoch: [0][1900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.70 (3.12)\tTrans Loss 9.68 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (15.3)\n",
      "Epoch: [0][2000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.28 (3.11)\tTrans Loss 9.71 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (15.7)\n",
      "Epoch: [0][2100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.60 (3.11)\tTrans Loss 9.69 (9.70)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (16.1)\n",
      "Epoch: [0][2200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.79 (3.10)\tTrans Loss 9.69 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (16.5)\n",
      "Epoch: [0][2300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.27 (3.10)\tTrans Loss 9.70 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (16.6)\n",
      "Epoch: [0][2400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.71 (3.09)\tTrans Loss 9.71 (9.69)\tCST Loss 0.09 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (17.0)\n",
      "Epoch: [0][2500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.17 (3.09)\tTrans Loss 9.70 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (17.4)\n",
      "Epoch: [0][2600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.87 (3.09)\tTrans Loss 9.69 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (17.6)\n",
      "Epoch: [0][2700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.19 (3.08)\tTrans Loss 9.68 (9.69)\tCST Loss 0.08 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (18.0)\n",
      "Epoch: [0][2800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.24 (3.07)\tTrans Loss 9.69 (9.69)\tCST Loss 0.07 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (18.4)\n",
      "Epoch: [0][2900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.69 (3.07)\tTrans Loss 9.68 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (18.7)\n",
      "Epoch: [0][3000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.69 (3.06)\tTrans Loss 9.70 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (19.1)\n",
      "Epoch: [0][3100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.51 (3.05)\tTrans Loss 9.68 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (19.4)\n",
      "Epoch: [0][3200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.29 (3.05)\tTrans Loss 9.68 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (19.4)\n",
      "Epoch: [0][3300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.09 (3.05)\tTrans Loss 9.71 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (19.8)\n",
      "Epoch: [0][3400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.98 (3.04)\tTrans Loss 9.68 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (20.0)\n",
      "Epoch: [0][3500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.94 (3.04)\tTrans Loss 9.68 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (20.3)\n",
      "Epoch: [0][3600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.88 (3.03)\tTrans Loss 9.68 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (20.5)\n",
      "Epoch: [0][3700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.88 (3.03)\tTrans Loss 9.66 (9.69)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (20.6)\n",
      "Epoch: [0][3800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.17 (3.03)\tTrans Loss 9.69 (9.69)\tCST Loss 0.07 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (20.6)\n",
      "Epoch: [0][3900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.10 (3.02)\tTrans Loss 9.69 (9.69)\tCST Loss 0.08 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (20.8)\n",
      "lr: 8.458970107524514e-05\n",
      "Epoch: [1][   0/4000]\tTime 0.2 (0.2)\tData 0.0 (0.0)\tLoss 2.90 (2.90)\tTrans Loss 9.68 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (0.0)\n",
      "Epoch: [1][ 100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.61 (2.75)\tTrans Loss 9.65 (9.68)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (34.7)\n",
      "Epoch: [1][ 200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.57 (2.82)\tTrans Loss 9.70 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (29.1)\n",
      "Epoch: [1][ 300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.67 (2.82)\tTrans Loss 9.62 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (31.6)\n",
      "Epoch: [1][ 400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.01 (2.81)\tTrans Loss 9.65 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (31.5)\n",
      "Epoch: [1][ 500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.54 (2.82)\tTrans Loss 9.67 (9.68)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (31.7)\n",
      "Epoch: [1][ 600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.91 (2.81)\tTrans Loss 9.70 (9.68)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (32.0)\n",
      "Epoch: [1][ 700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.05 (2.81)\tTrans Loss 9.68 (9.68)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (32.0)\n",
      "Epoch: [1][ 800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.29 (2.81)\tTrans Loss 9.68 (9.68)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (31.8)\n",
      "Epoch: [1][ 900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.00 (2.81)\tTrans Loss 9.67 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (32.4)\n",
      "Epoch: [1][1000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.98 (2.81)\tTrans Loss 9.69 (9.68)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (32.4)\n",
      "Epoch: [1][1100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.69 (2.80)\tTrans Loss 9.71 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (32.2)\n",
      "Epoch: [1][1200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.77 (2.80)\tTrans Loss 9.70 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (32.4)\n",
      "Epoch: [1][1300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.18 (2.80)\tTrans Loss 9.70 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (32.1)\n",
      "Epoch: [1][1400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.16 (2.80)\tTrans Loss 9.70 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (32.2)\n",
      "Epoch: [1][1500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.31 (2.79)\tTrans Loss 9.68 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (32.5)\n",
      "Epoch: [1][1600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.82 (2.79)\tTrans Loss 9.67 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (32.7)\n",
      "Epoch: [1][1700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.42 (2.79)\tTrans Loss 9.64 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (33.1)\n",
      "Epoch: [1][1800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.78 (2.79)\tTrans Loss 9.70 (9.68)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (33.1)\n",
      "Epoch: [1][1900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.13 (2.78)\tTrans Loss 9.70 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (33.3)\n",
      "Epoch: [1][2000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.05 (2.78)\tTrans Loss 9.68 (9.68)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (33.6)\n",
      "Epoch: [1][2100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.80 (2.78)\tTrans Loss 9.71 (9.68)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (33.8)\n",
      "Epoch: [1][2200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.90 (2.77)\tTrans Loss 9.70 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (33.9)\n",
      "Epoch: [1][2300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.11 (2.77)\tTrans Loss 9.65 (9.68)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (34.3)\n",
      "Epoch: [1][2400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.17 (2.77)\tTrans Loss 9.68 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (34.5)\n",
      "Epoch: [1][2500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.80 (2.77)\tTrans Loss 9.63 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (34.6)\n",
      "Epoch: [1][2600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.88 (2.76)\tTrans Loss 9.69 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (34.7)\n",
      "Epoch: [1][2700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.84 (2.76)\tTrans Loss 9.68 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (35.0)\n",
      "Epoch: [1][2800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.87 (2.76)\tTrans Loss 9.67 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (35.1)\n",
      "Epoch: [1][2900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.86 (2.75)\tTrans Loss 9.63 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (35.1)\n",
      "Epoch: [1][3000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.64 (2.75)\tTrans Loss 9.70 (9.68)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (35.2)\n",
      "Epoch: [1][3100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.12 (2.75)\tTrans Loss 9.68 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (35.3)\n",
      "Epoch: [1][3200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.57 (2.75)\tTrans Loss 9.58 (9.68)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (35.4)\n",
      "Epoch: [1][3300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.44 (2.74)\tTrans Loss 9.59 (9.68)\tCST Loss 0.07 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (35.5)\n",
      "Epoch: [1][3400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.56 (2.74)\tTrans Loss 9.68 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (35.6)\n",
      "Epoch: [1][3500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.25 (2.74)\tTrans Loss 9.66 (9.68)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (35.8)\n",
      "Epoch: [1][3600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.00 (2.74)\tTrans Loss 9.68 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (35.9)\n",
      "Epoch: [1][3700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.10 (2.73)\tTrans Loss 9.65 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (36.1)\n",
      "Epoch: [1][3800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.15 (2.73)\tTrans Loss 9.66 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (36.2)\n",
      "Epoch: [1][3900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.99 (2.73)\tTrans Loss 9.65 (9.68)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (36.3)\n",
      "lr: 7.377879464668812e-05\n",
      "Epoch: [2][   0/4000]\tTime 0.2 (0.2)\tData 0.0 (0.0)\tLoss 3.18 (3.18)\tTrans Loss 9.66 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (0.0)\n",
      "Epoch: [2][ 100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.50 (2.70)\tTrans Loss 9.69 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (36.6)\n",
      "Epoch: [2][ 200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.60 (2.64)\tTrans Loss 9.68 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (40.5)\n",
      "Epoch: [2][ 300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.04 (2.64)\tTrans Loss 9.67 (9.66)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (40.5)\n",
      "Epoch: [2][ 400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.78 (2.64)\tTrans Loss 9.58 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (40.8)\n",
      "Epoch: [2][ 500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.59 (2.63)\tTrans Loss 9.68 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (41.1)\n",
      "Epoch: [2][ 600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.54 (2.62)\tTrans Loss 9.67 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (42.0)\n",
      "Epoch: [2][ 700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.47 (2.62)\tTrans Loss 9.68 (9.66)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (41.9)\n",
      "Epoch: [2][ 800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.42 (2.62)\tTrans Loss 9.66 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (41.8)\n",
      "Epoch: [2][ 900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.03 (2.61)\tTrans Loss 9.61 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (42.2)\n",
      "Epoch: [2][1000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.39 (2.60)\tTrans Loss 9.69 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (43.0)\n",
      "Epoch: [2][1100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.57 (2.60)\tTrans Loss 9.69 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (43.2)\n",
      "Epoch: [2][1200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.82 (2.60)\tTrans Loss 9.70 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (43.7)\n",
      "Epoch: [2][1300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.29 (2.59)\tTrans Loss 9.69 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (43.7)\n",
      "Epoch: [2][1400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.18 (2.59)\tTrans Loss 9.69 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (43.8)\n",
      "Epoch: [2][1500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.98 (2.58)\tTrans Loss 9.65 (9.66)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.1)\n",
      "Epoch: [2][1600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.74 (2.58)\tTrans Loss 9.66 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (44.2)\n",
      "Epoch: [2][1700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.68 (2.58)\tTrans Loss 9.64 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.4)\n",
      "Epoch: [2][1800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.00 (2.58)\tTrans Loss 9.66 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.2)\n",
      "Epoch: [2][1900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.61 (2.58)\tTrans Loss 9.65 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.0)\n",
      "Epoch: [2][2000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.27 (2.58)\tTrans Loss 9.63 (9.66)\tCST Loss 0.11 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.2)\n",
      "Epoch: [2][2100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.77 (2.57)\tTrans Loss 9.67 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (44.3)\n",
      "Epoch: [2][2200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.39 (2.57)\tTrans Loss 9.64 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (44.3)\n",
      "Epoch: [2][2300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.82 (2.57)\tTrans Loss 9.67 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.4)\n",
      "Epoch: [2][2400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.29 (2.57)\tTrans Loss 9.69 (9.66)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (44.6)\n",
      "Epoch: [2][2500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.86 (2.57)\tTrans Loss 9.61 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (44.5)\n",
      "Epoch: [2][2600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.13 (2.57)\tTrans Loss 9.67 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (44.7)\n",
      "Epoch: [2][2700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.97 (2.57)\tTrans Loss 9.59 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.7)\n",
      "Epoch: [2][2800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.49 (2.57)\tTrans Loss 9.67 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.5)\n",
      "Epoch: [2][2900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.68 (2.56)\tTrans Loss 9.66 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.6)\n",
      "Epoch: [2][3000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.77 (2.57)\tTrans Loss 9.66 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.6)\n",
      "Epoch: [2][3100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.29 (2.57)\tTrans Loss 9.59 (9.66)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.5)\n",
      "Epoch: [2][3200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.37 (2.56)\tTrans Loss 9.71 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.6)\n",
      "Epoch: [2][3300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.01 (2.56)\tTrans Loss 9.61 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.8)\n",
      "Epoch: [2][3400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.73 (2.56)\tTrans Loss 9.59 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.8)\n",
      "Epoch: [2][3500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.32 (2.56)\tTrans Loss 9.53 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.7)\n",
      "Epoch: [2][3600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.42 (2.55)\tTrans Loss 9.63 (9.66)\tCST Loss 0.09 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.8)\n",
      "Epoch: [2][3700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.07 (2.55)\tTrans Loss 9.65 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (44.7)\n",
      "Epoch: [2][3800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.96 (2.55)\tTrans Loss 9.64 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (44.6)\n",
      "Epoch: [2][3900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.00 (2.55)\tTrans Loss 9.69 (9.66)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (44.7)\n",
      "lr: 6.572361810832016e-05\n",
      "Epoch: [3][   0/4000]\tTime 0.2 (0.2)\tData 0.0 (0.0)\tLoss 2.18 (2.18)\tTrans Loss 9.60 (9.60)\tCST Loss 0.08 (0.08)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.0)\n",
      "Epoch: [3][ 100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.92 (2.51)\tTrans Loss 9.63 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (43.1)\n",
      "Epoch: [3][ 200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.48 (2.51)\tTrans Loss 9.64 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (46.5)\n",
      "Epoch: [3][ 300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.44 (2.49)\tTrans Loss 9.65 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (49.2)\n",
      "Epoch: [3][ 400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.60 (2.50)\tTrans Loss 9.68 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.6)\n",
      "Epoch: [3][ 500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.85 (2.50)\tTrans Loss 9.61 (9.65)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.0)\n",
      "Epoch: [3][ 600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.72 (2.49)\tTrans Loss 9.68 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.1)\n",
      "Epoch: [3][ 700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.85 (2.50)\tTrans Loss 9.59 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (46.8)\n",
      "Epoch: [3][ 800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.37 (2.48)\tTrans Loss 9.62 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.9)\n",
      "Epoch: [3][ 900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.22 (2.48)\tTrans Loss 9.61 (9.65)\tCST Loss 0.07 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.8)\n",
      "Epoch: [3][1000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.39 (2.48)\tTrans Loss 9.62 (9.65)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.5)\n",
      "Epoch: [3][1100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.24 (2.47)\tTrans Loss 9.65 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.8)\n",
      "Epoch: [3][1200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.33 (2.47)\tTrans Loss 9.62 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.6)\n",
      "Epoch: [3][1300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.32 (2.47)\tTrans Loss 9.65 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.6)\n",
      "Epoch: [3][1400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.14 (2.47)\tTrans Loss 9.53 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.7)\n",
      "Epoch: [3][1500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.88 (2.47)\tTrans Loss 9.67 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.7)\n",
      "Epoch: [3][1600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.20 (2.47)\tTrans Loss 9.68 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.6)\n",
      "Epoch: [3][1700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.69 (2.47)\tTrans Loss 9.65 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.4)\n",
      "Epoch: [3][1800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.78 (2.46)\tTrans Loss 9.68 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.7)\n",
      "Epoch: [3][1900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.48 (2.47)\tTrans Loss 9.59 (9.64)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.3)\n",
      "Epoch: [3][2000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.41 (2.46)\tTrans Loss 9.67 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.3)\n",
      "Epoch: [3][2100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.19 (2.46)\tTrans Loss 9.65 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.3)\n",
      "Epoch: [3][2200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.17 (2.47)\tTrans Loss 9.63 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.2)\n",
      "Epoch: [3][2300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.11 (2.46)\tTrans Loss 9.55 (9.64)\tCST Loss 0.07 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.2)\n",
      "Epoch: [3][2400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.72 (2.47)\tTrans Loss 9.57 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.1)\n",
      "Epoch: [3][2500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.89 (2.46)\tTrans Loss 9.65 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.4)\n",
      "Epoch: [3][2600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.79 (2.46)\tTrans Loss 9.63 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.5)\n",
      "Epoch: [3][2700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.13 (2.46)\tTrans Loss 9.67 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.6)\n",
      "Epoch: [3][2800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.51 (2.46)\tTrans Loss 9.62 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.6)\n",
      "Epoch: [3][2900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.24 (2.46)\tTrans Loss 9.66 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.6)\n",
      "Epoch: [3][3000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.00 (2.46)\tTrans Loss 9.51 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.7)\n",
      "Epoch: [3][3100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.17 (2.46)\tTrans Loss 9.64 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.6)\n",
      "Epoch: [3][3200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.09 (2.46)\tTrans Loss 9.61 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.5)\n",
      "Epoch: [3][3300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.70 (2.46)\tTrans Loss 9.69 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.4)\n",
      "Epoch: [3][3400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.15 (2.46)\tTrans Loss 9.55 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.3)\n",
      "Epoch: [3][3500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.96 (2.46)\tTrans Loss 9.67 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.2)\n",
      "Epoch: [3][3600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.96 (2.46)\tTrans Loss 9.62 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (47.1)\n",
      "Epoch: [3][3700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.34 (2.46)\tTrans Loss 9.64 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.2)\n",
      "Epoch: [3][3800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.92 (2.46)\tTrans Loss 9.66 (9.64)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (47.3)\n",
      "Epoch: [3][3900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.61 (2.46)\tTrans Loss 9.68 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (47.3)\n",
      "lr: 5.946035575013605e-05\n",
      "Epoch: [4][   0/4000]\tTime 0.2 (0.2)\tData 0.0 (0.0)\tLoss 2.69 (2.69)\tTrans Loss 9.69 (9.69)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (0.0)\n",
      "Epoch: [4][ 100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.68 (2.43)\tTrans Loss 9.57 (9.64)\tCST Loss 0.10 (0.09)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (48.5)\n",
      "Epoch: [4][ 200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.72 (2.38)\tTrans Loss 9.67 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (50.7)\n",
      "Epoch: [4][ 300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.21 (2.39)\tTrans Loss 9.66 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (49.2)\n",
      "Epoch: [4][ 400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.49 (2.40)\tTrans Loss 9.66 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (48.8)\n",
      "Epoch: [4][ 500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.05 (2.40)\tTrans Loss 9.62 (9.64)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (48.5)\n",
      "Epoch: [4][ 600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.15 (2.40)\tTrans Loss 9.52 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (49.1)\n",
      "Epoch: [4][ 700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.62 (2.40)\tTrans Loss 9.54 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (49.0)\n",
      "Epoch: [4][ 800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.79 (2.40)\tTrans Loss 9.67 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (48.9)\n",
      "Epoch: [4][ 900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.91 (2.40)\tTrans Loss 9.66 (9.63)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (49.3)\n",
      "Epoch: [4][1000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.31 (2.40)\tTrans Loss 9.66 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (48.8)\n",
      "Epoch: [4][1100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.36 (2.40)\tTrans Loss 9.53 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (48.9)\n",
      "Epoch: [4][1200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.06 (2.40)\tTrans Loss 9.61 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (48.8)\n",
      "Epoch: [4][1300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.34 (2.39)\tTrans Loss 9.67 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (49.4)\n",
      "Epoch: [4][1400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.85 (2.39)\tTrans Loss 9.65 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (49.6)\n",
      "Epoch: [4][1500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.97 (2.39)\tTrans Loss 9.66 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.0)\n",
      "Epoch: [4][1600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.99 (2.39)\tTrans Loss 9.58 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.0)\n",
      "Epoch: [4][1700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.29 (2.39)\tTrans Loss 9.55 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.2)\n",
      "Epoch: [4][1800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.45 (2.39)\tTrans Loss 9.59 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (49.9)\n",
      "Epoch: [4][1900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.68 (2.39)\tTrans Loss 9.65 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (49.8)\n",
      "Epoch: [4][2000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.51 (2.39)\tTrans Loss 9.56 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.0)\n",
      "Epoch: [4][2100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.82 (2.39)\tTrans Loss 9.69 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.1)\n",
      "Epoch: [4][2200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.86 (2.39)\tTrans Loss 9.66 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (50.2)\n",
      "Epoch: [4][2300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.29 (2.39)\tTrans Loss 9.62 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (50.5)\n",
      "Epoch: [4][2400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.89 (2.39)\tTrans Loss 9.60 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (50.4)\n",
      "Epoch: [4][2500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 1.92 (2.39)\tTrans Loss 9.64 (9.63)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (50.5)\n",
      "Epoch: [4][2600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.20 (2.39)\tTrans Loss 9.56 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (50.4)\n",
      "Epoch: [4][2700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.66 (2.39)\tTrans Loss 9.53 (9.63)\tCST Loss 0.07 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.3)\n",
      "Epoch: [4][2800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.71 (2.39)\tTrans Loss 9.59 (9.63)\tCST Loss 0.07 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (50.3)\n",
      "Epoch: [4][2900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.55 (2.39)\tTrans Loss 9.61 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.6)\n",
      "Epoch: [4][3000/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.15 (2.38)\tTrans Loss 9.70 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 100.0 (50.5)\n",
      "Epoch: [4][3100/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.47 (2.38)\tTrans Loss 9.62 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (50.5)\n",
      "Epoch: [4][3200/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.74 (2.38)\tTrans Loss 9.45 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.6)\n",
      "Epoch: [4][3300/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.09 (2.38)\tTrans Loss 9.56 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.6)\n",
      "Epoch: [4][3400/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.40 (2.38)\tTrans Loss 9.52 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.7)\n",
      "Epoch: [4][3500/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.53 (2.38)\tTrans Loss 9.69 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (50.5)\n",
      "Epoch: [4][3600/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.11 (2.38)\tTrans Loss 9.67 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.5)\n",
      "Epoch: [4][3700/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.76 (2.38)\tTrans Loss 9.56 (9.63)\tCST Loss 0.08 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (50.5)\n",
      "Epoch: [4][3800/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 3.44 (2.38)\tTrans Loss 9.63 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 0.0 (50.6)\n",
      "Epoch: [4][3900/4000]\tTime 0.4 (0.5)\tData 0.2 (0.2)\tLoss 2.28 (2.38)\tTrans Loss 9.63 (9.63)\tCST Loss 0.10 (0.10)\tFix Loss 0.00 (0.00)\tCls Acc 50.0 (50.5)\n",
      "Test: [  0/511]\tTime  1.456 ( 1.456)\tLoss 9.3005e-01 (9.3005e-01)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n",
      "Test: [100/511]\tTime  0.066 ( 0.079)\tLoss 2.7836e-01 (1.1923e+00)\tAcc@1 100.00 ( 62.87)\tAcc@5 100.00 ( 99.01)\n",
      "Test: [200/511]\tTime  0.065 ( 0.072)\tLoss 9.2443e-01 (1.1781e+00)\tAcc@1 100.00 ( 60.95)\tAcc@5 100.00 ( 99.00)\n",
      "Test: [300/511]\tTime  0.065 ( 0.070)\tLoss 1.8201e+00 (1.1977e+00)\tAcc@1  50.00 ( 60.30)\tAcc@5 100.00 ( 98.67)\n",
      "Test: [400/511]\tTime  0.066 ( 0.069)\tLoss 1.0232e+00 (1.2279e+00)\tAcc@1  50.00 ( 59.10)\tAcc@5 100.00 ( 97.76)\n",
      "Test: [500/511]\tTime  0.065 ( 0.068)\tLoss 7.8077e-01 (1.2261e+00)\tAcc@1  50.00 ( 58.98)\tAcc@5 100.00 ( 97.90)\n",
      " * Acc@1 59.100 Acc@5 97.945\n",
      "\n",
      "Device-specific results:\n",
      "Device a: 59.10% (604/1022)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % args.val_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch == args.epochs - \u001b[32m1\u001b[39m:\n\u001b[32m     66\u001b[39m     acc_s, device_results_s = validate(val_source_loader, classifier, args)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     acc_t, device_results_t = \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_target_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# remember best acc@1 and save checkpoint\u001b[39;00m\n\u001b[32m     71\u001b[39m     torch.save(classifier.state_dict(), logger.get_checkpoint_path(\u001b[33m'\u001b[39m\u001b[33mlatest\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mvalidate\u001b[39m\u001b[34m(val_loader, model, args)\u001b[39m\n\u001b[32m     21\u001b[39m end = time.time()\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (audio, target, _, devices) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     audio = \u001b[43maudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(args.device)\n\u001b[32m     24\u001b[39m     target = target.to(args.device)\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# compute output\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Arguments set for CST experiment (DCASE TAU 2020):\n",
      "root = ./data/dcase\n",
      "data = DCASE\n",
      "source = source\n",
      "target = target\n",
      "temperature = 2.0\n",
      "alpha = 1.9\n",
      "trade_off = 0.08\n",
      "trade_off1 = 0.5\n",
      "trade_off3 = 0.5\n",
      "threshold = 0.97\n",
      "rho = 0.5\n",
      "batch_size = 2\n",
      "lr = 0.001\n",
      "lr_gamma = 0.001\n",
      "lr_decay = 0.75\n",
      "momentum = 0.9\n",
      "weight_decay = 0.001\n",
      "workers = 8\n",
      "epochs = 60\n",
      "early = 50\n",
      "iters_per_epoch = 4000\n",
      "val_interval = 5\n",
      "print_freq = 100\n",
      "seed = None\n",
      "per_class_eval = False\n",
      "log = logs/dcase\n",
      "phase = train\n",
      "sample_rate = 32000\n",
      "clip_length = 10\n",
      "num_cls = 10\n",
      "device = cuda\n",
      "gradient_accumulation_steps = 16\n",
      "Train source samples: 9193\n",
      "Train target samples: 3372\n",
      "Val source samples: 1022\n",
      "Val target samples: 375\n",
      "Test source samples: 330\n",
      "Test target samples: 2638\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d977c8-ce61-4f0e-a876-500a26c85352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
